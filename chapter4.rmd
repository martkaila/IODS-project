# Chapter 4

Today is 
```{r}
date()
```

## Analysis exercise 

## 1. Create new Rmarkdown file 
done!

## 2. Load the Boston Data

Let us load the data and then summarize and provide a glimpse into the data.

```{r, warning=FALSE, message = FALSE}
library(dplyr); library (MASS)
glimpse(Boston) 
summary(Boston)
```

The data contain 506 observations, which each apparently represent different census tracts in Boston. The data have 14 variables that describe various characteristics of these census tracts like crime rate, tax rate, and a median value of homes in the census tract.  

## 3. Graphical overview

I start by drawing some scatter plots. 

```{r, warning=FALSE, message = FALSE}
library(tidyr); library(dplyr); library(ggplot2); library(GGally)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
```

### 3.1 Histograms
Since the data contain so many variables, most of the histograms are relatively small. However, we can at least say that there seems to be quite a bit of variation in age, nox and medv variables.
### 3.1 Scatter and correlations plots

```{r, warning=FALSE, message = FALSE}
library("ggplot2"); library("GGally");library(corrplot)
pairs(Boston)
cMatrix = cor(Boston) %>% round(digits=2)
corrplot(cMatrix, method = "ellipse", type= "upper")

```

Since the correlation plot with scatter plots is messy, I will concentrate just on the latter figure, which shows only the correlations between variables. Based on the figure, some variables like lower statuts of the population (lstat) and median value of homes (medv) are very negatively correlated, some variables like proportion of non-retail business (indus) and emissons (nox) are very positively correlated, whereas between some variables like there do not exist any relationship.

## 4. Standardize the data

### 4.1 Standardization 

```{r, warning=FALSE, message = FALSE}
library("ggplot2"); library("GGally");library(corrplot)
bostonStand <-scale(Boston)
bostonStand <- as.data.frame(bostonStand)
summary(bostonStand)
```
We can see that after the standardization procedure, each variable is centered around zero. (mean is zero). Let us then create a new categorical variable using the existing crime variable. 

```{r, warning=FALSE, message = FALSE}
library("ggplot2"); library("GGally");library(corrplot); library(dplyr)
bins <- quantile(bostonStand$crim)
crime <-cut(bostonStand$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high") )
bostonStand <- dplyr::select(bostonStand , -crim)
summary(bostonStand)
bostonStand$crime <- crime
summary(bostonStand)
```

### 4.2 Train and test sets

Let us create training and test sets and then have a glimpse at both of them.

```{r, warning=FALSE, message = FALSE}
library("ggplot2");library(corrplot); library(dplyr)
nObs <- nrow(bostonStand)
indTrain <- sample(nObs , size = nObs * 0.8)
# Training set 
trainBoston <- bostonStand[indTrain,]
# Test set 
testBoston <- bostonStand[-indTrain,]

glimpse(trainBoston) 
glimpse(testBoston) 
```

## 5. Discriminant analysis 

```{r, warning=FALSE, message = FALSE}
library("ggplot2");library(corrplot); library(dplyr)

lda.fit <- lda( crime ~ . , data = trainBoston )

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(trainBoston$crime)

plot(lda.fit , dimen = 2,  col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

## 6. Predict and cross tabulate

Let us first save the initial values of the crime variable of test data into a separate vector.

```{r, warning=FALSE, message = FALSE}
library("ggplot2");library(corrplot); library(dplyr)

# save the initial values of crime vectors 
orginalCrime <- testBoston$crime 

testBoston <- dplyr::select(testBoston, -crime)

```


Then let us predict the values of the crime variable in the test data using the model we found using the training data. Finally, we compare the predicted and actual values of the crime variable.

```{r, warning=FALSE, message = FALSE}
library("ggplot2");library(corrplot); library(dplyr)

lda.pred <- predict(lda.fit, newdata =  testBoston)

table(correct = orginalCrime, lda.pred$class )

```

The diagonal line from the left top corner to the bottom right corner shows the number of correct predictions. Based on the table, our model performs ok. The model does very well at predicting extreme values (low and high crimes) while there seem to be more wrong guesses when predicting values are closer to the median.  
  

## 7. Distances and K-means algorithm 

### 7.1 Distances
First we re-load the data and standardize the variables. Then I calculate euclidean distances between variables.
```{r, warning=FALSE, message = FALSE}
library(dplyr); library (MASS)
boston<-scale(Boston)
boston <- as.data.frame(boston)

distE <- dist(boston)
summary(distE )

```
### 7.2 K-means algorithm 

I first perform k-mean clustering with 7 clusters. Note that I pick the number of clusters just randomly. 

```{r, warning=FALSE, message = FALSE}
library(dplyr); library (MASS)
kM <- kmeans(Boston, centers = 7)
pairs(boston[1:5], col = kM$cluster )
pairs(boston[6:10], col = kM$cluster )
pairs(boston[11:14], col = kM$cluster )
```

Let us then be a bit formal and find the optimal number of clusters by calculating the total of within-cluster sum of squares (WCSS).

```{r, warning=FALSE, message = FALSE}
library(dplyr); library (MASS); library("ggplot2");
set.seed(123)

k_max <- 10

twcss <- sapply(1:k_max, function(k){kmeans(boston, k)$tot.withinss})

qplot(x = 1:k_max, y = twcss, geom = 'line')

```

Based on the figure, it seems that the WCSS drops heavily when the number of clusters is 2. Let us perform the k-mean clustering using two clusters and plot the results 


```{r, warning=FALSE, message = FALSE}
library(dplyr); library (MASS)
kM <- kmeans(Boston, centers = 2)
pairs(boston[1:5], col = kM$cluster )
pairs(boston[6:10], col = kM$cluster )
pairs(boston[11:14], col = kM$cluster )
```


